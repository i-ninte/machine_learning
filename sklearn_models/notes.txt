## first tutorial on sklearn 
#
'''
the number of features is called dimensions
features are the columns used they act as input to the model and are used to determine the right label. 
the labels depend on the features. they are the output of the features 

features, attributes, independent variables, input
labels, output, dependent variable


the number of rows ==number of instances 
features are represented by X and labels by y
features must be between   the range of -1 and 1



SAVING THE MODEL 
fom sklearn.externals import joblib 

then after training the model we create a filename with a .sav extension
filename= 'model.sav'
joblib.dump(clf, filenamme) clf cos it is a classification model we intend to use in this case.



OPENING THE MODEL
remove all the codes used in savibng the model and 
clf= joblib.load(filename)




CLASSIFICATION
grouping similar features under one label


TRAIN TEST SPLIT
 for splitting the data into test and training data
 we include from sklearn.model_selection import train_test_split
 
 
 then we create the variables X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.2)
this indicates that the size of the data used for training is 20%
'''


SVM
support vector machine is used in high dimensional cases or cases where there are many features.

AREAS OF USAGE
Effective high-dimensional spaces 
many kernel functions
classification and regression

it calculates the distance fromthe support vectors to the machine.


kERNELS 
are functions we use to increase the dimension
we use it for classification and regressions
rbf - polynomials



LINEAR REGRESSION

the r squared value indicates how accurate the egression line represents tthe  data points


LOGISTIC REGRESSION 
it is a sigmoid function  logit(y) = 1/(1+pow(e,-y))

it converts the values from the range of zero to one.


KMEANS AND THE MATHS BEHIND IT 
clustering is an algorithmn where data points ae assigned to labels based on feature similarity
Kmeans uses centroids 
number of labels == number of centroids
the algorithmn then finds a plane equidistant to the centroids.
then it calculates the average position of all poimts and separates the dataset
the palne sepaating the centroids are called hyperpoints

the diffenece between clustering and classification is that clustering does not train models whiles classification does. 

In fitting you do not pass in the y_train because clustering takes the features and separates them by itself
