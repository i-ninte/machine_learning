# -*- coding: utf-8 -*-
"""BA prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VBzx-sxx6YeqE_tstOsOeIyZM7d7xWXk

Here is the background information on your task
Customers are more empowered than ever because they have access to a wealth of information at their fingertips. This is one of the reasons the buying cycle is very different to what it used to be. Today, if you’re hoping that a customer purchases your flights or holidays as they come into the airport, you’ve already lost! Being reactive in this situation is not ideal; airlines must be proactive in order to acquire customers before they embark on their holiday.

This is possible with the use of data and predictive models. The most important factor with a predictive model is the quality of the data you use to train the machine learning algorithms. For this task, you must manipulate and prepare the provided customer booking data so that you can build a high-quality predictive model.

With your predictive model, it is important to interpret the results in order to understand how “predictive” the data really was and whether we can feasibly use it to predict the target outcome (customers buying holidays). Therefore, you should evaluate the model's performance and output how each variable contributes to the predictive model's power.

###Build a predictive model to understand factors that influence buying behaviour

##Here is your task
##Explore and prepare the dataset
First, spend some time exploring the dataset in the “Getting Started” Jupyter Notebook provided in the Resources section below to understand the different columns and some basic statistics of the dataset. Then, you should consider how to prepare the dataset for a predictive model. You should think about any new features you want to create in order to make your model even better. You can make use of the Resources provided to get you started with this task.

##Train a machine learning model
When your data is ready for modelling, you should train a machine learning model to be able to predict the target outcome, which is a customer making a booking. For this task, you should use an algorithm that easily allows you to output information about how each variable within the model contributes to its predictive power. For example, a RandomForest is very good for this purpose.

##Evaluate model and present findings
After training your model, you should evaluate how well it performed by conducting cross-validation and outputting appropriate evaluation metrics. Furthermore, you should create a visualisation to interpret how each variable contributed to the model. Finally, you should summarise your findings in a single slide to be sent to your manager. Use the “PowerPoint Template” provided in the Resources section below to create your summary and make use of the links provided to help with this task.

It is recommended that the analysis portion of this task is done in Python.

Once you’ve completed your PowerPoint, please submit your document below.

# Task 2

---

## Predictive modeling of customer bookings

This Jupyter notebook includes some code to get you started with this predictive modeling task. We will use various packages for data manipulation, feature engineering and machine learning.

### Exploratory data analysis

First, we must explore the data in order to better understand what we have and the statistical properties of the dataset.
"""

import pandas as pd

df = pd.read_csv("customer_booking.csv", encoding="ISO-8859-1")
df.head()

df.info()

"""The `.info()` method gives us a data description, telling us the names of the columns, their data types and how many null values we have. Fortunately, we have no null values. It looks like some of these columns should be converted into different data types, e.g. flight_day.

To provide more context, below is a more detailed data description, explaining exactly what each column means:

- `num_passengers` = number of passengers travelling
- `sales_channel` = sales channel booking was made on
- `trip_type` = trip Type (Round Trip, One Way, Circle Trip)
- `purchase_lead` = number of days between travel date and booking date
- `length_of_stay` = number of days spent at destination
- `flight_hour` = hour of flight departure
- `flight_day` = day of week of flight departure
- `route` = origin -> destination flight route
- `booking_origin` = country from where booking was made
- `wants_extra_baggage` = if the customer wanted extra baggage in the booking
- `wants_preferred_seat` = if the customer wanted a preferred seat in the booking
- `wants_in_flight_meals` = if the customer wanted in-flight meals in the booking
- `flight_duration` = total duration of flight (in hours)
- `booking_complete` = flag indicating if the customer completed the booking

Before we compute any statistics on the data, lets do any necessary data conversion
"""

df["flight_day"].unique()

mapping = {
    "Mon": 1,
    "Tue": 2,
    "Wed": 3,
    "Thu": 4,
    "Fri": 5,
    "Sat": 6,
    "Sun": 7,
}

df["flight_day"] = df["flight_day"].map(mapping)

df["flight_day"].unique()

df.describe()

"""The `.describe()` method gives us a summary of descriptive statistics over the entire dataset (only works for numeric columns). This gives us a quick overview of a few things such as the mean, min, max and overall distribution of each column.

From this point, you should continue exploring the dataset with some visualisations and other metrics that you think may be useful. Then, you should prepare your dataset for predictive modelling. Finally, you should train your machine learning model, evaluate it with performance metrics and output visualisations for the contributing variables. All of this analysis should be summarised in your single slide.
"""

df.shape

"""##Sales Channel"""

per_internet = (df.sales_channel.value_counts().values[0] / df.sales_channel.count()) * 100
per_mobile = (df.sales_channel.value_counts().values[1] / df.sales_channel.count()) * 100

print("percentage of bookings made through the internet: ", per_internet)
print("percentage of bookings made through phone calls: ", per_mobile)

"""Most bookings were made over the internet

#Trip type
"""

per_round= df.trip_type.value_counts().values[0]/df.trip_type.count() * 100
per_oneway= df.trip_type.value_counts().values[1]/df.trip_type.count() * 100
per_circle= df.trip_type.value_counts().values[2]/df.trip_type.count() * 100

print("percentage of round trips: ", per_round)
print("percentage of one way trips: ", per_oneway)
print("percentage of circle trips: ", per_circle)

"""##Purchase Lead"""

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(15,5))
sns.histplot(data=df, x="purchase_lead", binwidth=20, kde=True)

"""here are a few bookings made more than 2 years before the travel date, which seems unlikely as they were not likely to be made in advance. These outliers could be due to customer cancellations and subsequent rebookings within a 6-month period"""

(df.purchase_lead > 600).value_counts()

"""Let's assume that no customer makes a booking more than a year and a half in advance. Therefore, we will remove all purchase_lead times greater than 600 days."""

df[df.purchase_lead > 600]

df= df[df.purchase_lead < 600]

df.columns

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = df.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

"""##Predictive Model"""

df.reset_index(drop=True)

"""#SCALING"""

df['sales_channel']

# Replace values in the "sales_channel" column
df['sales_channel'].replace({'Mobile': 0, 'Internet': 1}, inplace=True)

df['trip_type'].replace({'RoundTrip': 0, 'OneWay': 1, 'CircleTrip':2}, inplace=True)

df_final = df.drop(["route", "booking_origin"], axis=1)

from sklearn.preprocessing import StandardScaler

scaler= StandardScaler()

scaled_df= scaler.fit_transform(df_final)

#create a dataframe of scaled data
scaled_df= pd.DataFrame(scaled_df, columns=df_final.columns)

scaled_df.head()



label= df['booking_complete']

df['booking_complete']

scaled_df.drop('booking_complete', axis=1, inplace=True)

scaled_df['label']= label

scaled_df.head()

"""#splitting Train and Test Data"""

from sklearn.model_selection import train_test_split
X= scaled_df.iloc[:,:-1]
y= scaled_df['label']

X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.2,random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.inspection import permutation_importance

from yellowbrick.classifier import ConfusionMatrix
from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold

'''
create a function to fir and predict whether the customer would complete the booking and also for evaluation of the prediction model

'''


def model_fit_predict(model, X, y,X_predict):
  model.fit(X,y)
  return model.predict(X_predict)


def acc_score(y_true, y_pred):
  return accuracy_score(y_true,y_pred)

def pre_score(y_true, y_pred):
  return precision_score(y_true, y_pred)


def f_score(y_true, y_pred):
  return f1_score(y_true,y_pred)

"""##Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

# Create a RandomForestClassifier instance
clf_rf = RandomForestClassifier()

# Fit the classifier to your scaled data
clf_rf.fit(X_train,y_train)

"""###checking accuracy"""

y_pred_train = model_fit_predict(clf_rf, X_train, y_train, X_train)
set(y_pred_train)

# F1 score for training data
f1 = round(f1_score(y_train, y_pred_train), 2)

# Accuracy score of training data
acc = round(accuracy_score(y_train, y_pred_train), 2)

# Precision score for training data
pre = round(pre_score(y_train, y_pred_train), 2)

# Convert the floating-point numbers to strings before concatenating
print("accuracy, f1_score, and precision for training data respectively:", str(acc) + " " + str(f1) + " " + str(pre))

cm= ConfusionMatrix(clf_rf, classes=[0,1])
cm.fit(X_train, y_train)

cm.score(X_train, y_train)

"""##Checking testing accuracy"""

#create an instance of the classifier and fit the testing data
clf_rf= RandomForestClassifier(max_depth=50, min_samples_split=5, random_state=0)

y_pred_test= model_fit_predict(clf_rf, X_train,y_train, X_test)

# F1 score for training data
f1_1= round(f1_score(y_test, y_pred_test), 2)

# Accuracy score of training data
acc_1 = round(accuracy_score(y_test, y_pred_test), 2)

# Precision score for training data
pre_1 = round(pre_score(y_test, y_pred_test), 2)

# Convert the floating-point numbers to strings before concatenating
print("accuracy, f1_score, and precision for training data respectively:", str(acc_1) + " " + str(f1_1) + " " + str(pre_1))

cm= ConfusionMatrix(clf_rf, classes=[0,1])
cm.fit(X_test, y_test)

cm.score(X_test, y_test)

plt.figure(figsize=(10,5))
sorted_idx= clf_rf.feature_importances_.argsort()
plt.barh(scaled_df.iloc[:,:-1].columns[sorted_idx], clf_rf.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")

"""One major problem behind low f1 scoe is imbalancing dataset. We have higher enteries that are classified 0 than 1
We should reduce the number of enteries that are classified 0 to be equal to number of enteries that are classified  1.

##Balancing the dataset
"""

scaled_df.label.value_counts()

##create a dataframe having all labels 0 with 1000 samples
scaled_df_0= scaled_df[scaled_df.label==0].sample(n=8000)

#concatenate the two dataframe, one having all labels as 1
scaled_df_new= pd.concat([scaled_df[scaled_df.label==1], scaled_df_0], ignore_index=True)

#shuffle the dataframe rows
scaled_df_new= scaled_df_new.sample(frac=1).reset_index(drop=True)

scaled_df_new

X= scaled_df_new.iloc[:,:-1]
y= scaled_df_new['label']
X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.2, random_state=0)

#create an instance of the classifier and fit the testing data
clf_rf= RandomForestClassifier(max_depth=50, min_samples_split=5, random_state=0)

y_pred_test= model_fit_predict(clf_rf, X_train,y_train, X_test)

# F1 score for training data
f1_1= round(f1_score(y_test, y_pred_test), 2)

# Accuracy score of training data
acc_1 = round(accuracy_score(y_test, y_pred_test), 2)

# Precision score for training data
pre_1 = round(pre_score(y_test, y_pred_test), 2)

# Convert the floating-point numbers to strings before concatenating
print("accuracy, f1_score, and precision for training data respectively:", str(acc_1) + " " + str(f1_1) + " " + str(pre_1))

cm= ConfusionMatrix(clf_rf, classes=[0,1])
cm.fit(X_test, y_test)

cm.score(X_test, y_test)

plt.figure(figsize=(10,5))
sorted_idx= clf_rf.feature_importances_.argsort()
plt.barh(scaled_df.iloc[:,:-1].columns[sorted_idx], clf_rf.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")
