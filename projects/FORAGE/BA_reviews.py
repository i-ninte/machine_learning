# -*- coding: utf-8 -*-
"""BA_REVIEWS_PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nQUman4gYx7DtSfuuUTrdTqgUUzYZ-Mx
"""

pip install requests

pip install beautifulsoup4

"""British Airways (BA) is the flag carrier airline of the United Kingdom (UK). Every day, thousands of BA flights arrive to and depart from the UK, carrying customers across the world. Whether it’s for holidays, work or any other reason, the end-to-end process of scheduling, planning, boarding, fuelling, transporting, landing, and continuously running flights on time, efficiently and with top-class customer service is a huge task with many highly important responsibilities.

As a data scientist at BA, it will be your job to apply your analytical skills to influence real life multi-million-pound decisions from day one, making a tangible impact on the business as your recommendations, tools and models drive key business decisions, reduce costs and increase revenue.

Customers who book a flight with BA will experience many interaction points with the BA brand. Understanding a customer's feelings, needs, and feedback is crucial for any business, including BA.

This first task is focused on scraping and collecting customer feedback and reviewing data from a third-party source and analysing this data to present any insights you may uncover.

##Scrape data from the web
  The first thing to do will be to scrape review data from the web. For this, you should use a website called Skytrax[https://www.airlinequality.com/]

The team leader wants you to focus on reviews specifically about the airline itself. You should collect as much data as you can in order to improve the output of your analysis. To get started with the data collection, you can use the “Jupyter Notebook” in the Resources section below to run some Python code that will help to collect some data.

##Analyse data
Once you have your dataset, you need to prepare it. The data will be very messy and contain purely text. You will need to perform data cleaning in order to prepare the data for analysis. When the data is clean, you should perform your own analysis to uncover some insights. As a starting point, you could look at topic modelling, sentiment analysis or wordclouds to provide some insight into the content of the reviews. It is recommended to complete this task using Python, however, you can use any tool that you wish. You can use some of the documentation websites provided in the Resources section below to analyse the data.



##Present insights
Your manager would like you to summarise your findings within a single PowerPoint slide, so that they can present the results at the next board meeting. You should create visualisations and metrics to include within this slide, as well as clear and concise explanations in order to quickly provide the key points from your analysis. Use the “PowerPoint Template” provided to complete the slide.

#DATA COLLECTION
"""

import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import requests
page_size=100
# Create empty lists to collect reviews and other information
reviews = []
stars = []
date = []
country = []
traveler_type = []
seat_type = []
seat_comfort = []
cabin_staff_service = []
food_and_beverages = []
inflight_environment = []
ground_service = []
wifi_and_internet = []

for i in range(1, 36):
    page = requests.get(f"https://www.airlinequality.com/airline-reviews/british-airways/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}")
    soup = BeautifulSoup(page.content, "html5")
    for item in soup.find_all("div", class_="text_content"):
        reviews.append(item.text)

    # Rating
    for item in soup.find_all("div", class_="rating-10"):
        try:
            stars.append(item.span.text)
        except:
            print("Error on page:", i)
            stars.append("None")

    # Date
    for item in soup.find_all("time"):
        date.append(item.text)

    # Country
    for item in soup.find_all("h3"):
        country.append(item.span.next_sibling.text.strip(" ()"))

    # Additional information
    for item in soup.find_all("tr"):
        if "Type Of Traveller" in item.text:
            traveler_type.append(item.find("td").text)
        if "Seat Type" in item.text:
            seat_type.append(item.find("td").text)
        if "Seat Comfort" in item.text:
            seat_comfort.append(item.find("td").text)
        if "Cabin Staff Service" in item.text:
            cabin_staff_service.append(item.find("td").text)
        if "Food & Beverages" in item.text:
            food_and_beverages.append(item.find("td").text)
        if "Inflight Entertainment" in item.text:
            inflight_environment.append(item.find("td").text)
        if "Ground Service" in item.text:
            ground_service.append(item.find("td").text)
        if "Wifi & Internet" in item.text:
            wifi_and_internet.append(item.find("td").text)

#check the length of the total reviews
len(reviews)

len(country)

#check length  of stars
stars = stars[:3500]

len(stars)

len(traveler_type)

len(seat_type)

len(seat_comfort)

len(cabin_staff_service)

len(food_and_beverages)

len(inflight_environment)

len(ground_service)

len(wifi_and_internet)

# Determine the list with the least length
min_length = min(len(stars), len(traveler_type), len(seat_type), len(seat_comfort), len(cabin_staff_service), len(food_and_beverages), len(inflight_environment), len(ground_service))

# Extend the lists to be the same length (3500) with zeros as placeholders
stars = stars[:min_length] + [0] * (3500 - min_length)
traveler_type = traveler_type[:min_length] + [0] * (3500 - min_length)
seat_type = seat_type[:min_length] + [0] * (3500 - min_length)
seat_comfort = seat_comfort[:min_length] + [0] * (3500 - min_length)
cabin_staff_service = cabin_staff_service[:min_length] + [0] * (3500 - min_length)
food_and_beverages = food_and_beverages[:min_length] + [0] * (3500 - min_length)
inflight_environment = inflight_environment[:min_length] + [0] * (3500 - min_length)
ground_service = ground_service[:min_length] + [0] * (3500 - min_length)

# Create a DataFrame from the collected data
df = pd.DataFrame({
    "reviews": reviews,
    "stars": stars,
    "date": date,
    "country": country,

})

df.to_csv("BA_reviews.csv")

df= pd.read_csv("BA_reviews.csv",index_col=0)

df.shape

df.head()

df.tail()

"""#DATA CLEANING

creating a column for verification
"""

df['verified']=df.reviews.str.contains("trip verified")

df['verified']

"""cleaning reviews"""

df.head()

import nltk

nltk.download('stopwords')

nltk.download('wordnet')

from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
lemma= WordNetLemmatizer()


reviews_data= df.reviews.str.strip("✅ Trip Verified |")

#create an empty list to collect cleaned data corpus
corpus=[]

#loop though each review, rename punctuations,small case it, join it and it to corpus
for rev  in reviews_data:
  rev = re.sub('[^a-zA-Z]', ' ',rev)
  rev= rev.lower()
  rev= rev.split()
  rev= [lemma.lemmatize(word) for word in rev if word not in set(stopwords.words("english"))]
  rev=" ".join(rev)
  corpus.append(rev)

#add the corpus to the original dataframe
df['corpus']= corpus

df.head()

"""##cleaning / fomatting date"""

df.dtypes

#convert the date to datetime format
df.date= pd.to_datetime(df.date)

df.date.head()

df.dtypes

"""##cleaning ratings with stars"""

#checking for unique values
df.stars.unique()

#remove the \t and \n fromthe ratings
df.stars= df.stars.str.strip("\n\t\t\t\t\t\t\t\t\t\t\t\t\t")

df.head()

df.stars.value_counts()

df.stars.unique()

"""##checking for nulls"""

df.isnull().sum()

"""we have 2 nulls in the country column"""

df.drop(df[df.country.isnull() == True].index, axis=0, inplace=True)

df.shape

#resrtting the index
df.reset_index(drop=True)

df.isnull().sum()

"""#Exploratory Data Analysis"""

#imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import datetime as dt
from wordcloud import WordCloud, STOPWORDS

df.head()

import nltk
from nltk.corpus import stopwords

#start with a review
reviews= " ".join(df.corpus)
plt.figure(figsize=(20,10))

stopwords= set(stopwords.words("english"))

#create and generate a word cloud image:
wordcloud =WordCloud(height=600, max_font_size=500, stopwords=stopwords).generate(reviews)


#displaying the generated image
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""there are many  words in the wordcloud but then not all of them tell us about how positive or negative the flight was"""

import nltk
from nltk.corpus import stopwords

#start with a review
reviews= " ".join(df.corpus)
plt.figure(figsize=(20,10))

stopwords= set(stopwords.words("english"))
stopwords.update(['ba','flight','airway','airline','plane','told','also','took','passenger','london','heathrow','aircraft','could','even','would'])
#create and generate a word cloud image:
wordcloud =WordCloud(height=600, width=1000,max_font_size=100,max_words=300, stopwords=stopwords).generate(reviews)


#displaying the generated image
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""##Word Fequency"""

from nltk import ngrams
from nltk.probability import FreqDist

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer

#split the text of all reviews into a list of words
words= reviews.split(" ")

#remove certain words that will not be used to determine the positive or negative sentiment
stopwords= text.ENGLISH_STOP_WORDS.union(['flight','bn','passenger',"u",'london','airway','british','heathrow','plane','1hr','review'])

new_words=[word for word in words if word not in stopwords]
nlp_words=FreqDist(new_words).most_common(20)

#create a panda series of these words and it's frequencies
all_fdist= pd.Series(dict(nlp_words))

from nltk import ngrams
from nltk.probability import FreqDist

from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer

import matplotlib.pyplot as plt  # Import matplotlib for plotting

# Split the text of all reviews into a list of words
words = reviews.split(" ")

# Remove certain words that will not be used to determine the positive or negative sentiment
stopwords = text.ENGLISH_STOP_WORDS.union(['flight', 'bn', 'passenger', "u", 'london', 'airway', 'british', 'heathrow', 'plane', '1hr', 'review','ba'])

new_words = [word for word in words if word not in stopwords]
nlp_words = FreqDist(new_words).most_common(20)

# Create a panda series of these words and their frequencies
all_fdist = pd.Series(dict(nlp_words))

# Create a bar chart
plt.figure(figsize=(12, 6))
colors = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon', 'lightseagreen', 'lightpink', 'lightsteelblue', 'lightyellow', 'lightblue', 'lightskyblue']

ax = all_fdist.plot(kind='bar', color=colors)
ax.set_ylabel('Frequency')
ax.set_xlabel('Words')

# Display exact frequencies on top of the bars
for i, v in enumerate(all_fdist):
    ax.text(i, v + 5, str(v), ha='center', va='bottom')

plt.title('Word Frequency Bar Chart')
plt.xticks(rotation=45, ha='right')

plt.show()

"""This distribution gives us a glimpse of what the customers are really talking about. We see that "Seat" is the most talked about followed by "service", "time" and "food". However we do not know whether the remarks are positive or not. To see this I will use the ngrams plot to see if they are good or bad.

##Word Frequency  with Ngram
"""

#imports
import nltk.collocations as collocations
from nltk import FreqDist, bigrams


reviews=" ".join(df.corpus)

#split the text of all reviews into a list of words
words= reviews.split(" ")

new_words= [word for word in words if word not in stopwords]


def get_freq_dist(new_words, number_of_ngrams):
  from nltk import ngrams

  #generate bigrams
  ngrams= ngrams(new_words, number_of_ngrams)

  #creating FreqDist
  ngram_fd= FreqDist(ngrams).most_common(40)


  #sort values by highest frequency
  ngrams_sorted= {k:v for k,v in sorted(ngram_fd, key=lambda item:item[1])}

  #join bigrams tokens with "_" and maintain sorting
  ngram_joined= {'_'.join(k):v for k,v in sorted(ngram_fd, key=lambda item:item[1])}

  #convert to panda series for easy plotting
  ngram_freqdist= pd.Series(ngram_joined)
  plt.figure(figsize=(10,10))
  ax= ngram_freqdist.plot(kind="barh")

  return ax


get_freq_dist(new_words,4)

"""We can see that there are very common positiv terms regarding the cabin crew as shown on the chart cabin_crew_friendly_attentive,cabin_crew_friendly_helpful, and cabin_crew_efficient_friendly. These are  certainly good remarks about the cabin crew staff of British Airways.

However, there is another approach that we can use which will provide better information about the reviews. We categorize ratings 1-3 as bad, 4-6 as average/good and 7-10 as great experiences.
"""

ratings_1_3= df[df.stars.isin([1,2,3])]
ratings_4_6= df[df.stars.isin([4,5,6])]
ratings_7_10= df[df.stars.isin([7,8,9,10])]

reviews_1_3= " ".join(ratings_1_3.corpus)
reviews_4_6= " ".join(ratings_4_6.corpus)
reviews_7_10= " ".join(ratings_7_10.corpus)

#split the text of all reviews into a list of words
words_1_3= reviews_1_3.split(" ")
words_4_6= reviews_4_6.split(" ")
words_7_10= reviews_7_10.split(" ")

new_words= [word for word in words if word not in stopwords]
get_freq_dist(new_words, 4)

new_words_4_6= [word for word in words_4_6 if word not in stopwords]
get_freq_dist(new_words_4_6, 4)

"""Now we use textblob to define whether the text is negative or positive"""

import nltk

nltk.download('movie_reviews')

nltk.download('punkt')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# from textblob import TextBlob
# from textblob.sentiments import NaiveBayesAnalyzer
# 
# #set a column polarity with all 0 values initially
# df['polarity']=0
# 
# 
# for i in range(len(df.corpus)):
#   sent= TextBlob(df.corpus[i])
#   polarity = sent.sentiment.polarity
#   subjectivity= sent.sentiment.subjectivity
#   df['polarity'][i]= polarity

df['polarity']

import pandas as pd


country_review_counts = df['country'].value_counts()

# Find the country with the maximum number of reviews
max_reviews_country = country_review_counts.idxmax()

# Display the maximum number of reviews and the corresponding country
print(f"The country with the maximum number of reviews is '{max_reviews_country}' with {country_review_counts[max_reviews_country]} reviews.")

import pandas as pd
import matplotlib.pyplot as plt


country_review_counts = df['country'].value_counts()

# Get the top 5 countries with the most reviews
top_5_countries = country_review_counts.head(5)

# Create a bar chart to visualize the distribution
plt.figure(figsize=(10, 6))
top_5_countries.plot(kind='bar', color='skyblue')
plt.title('Top 5 Countries by Number of Reviews')
plt.xlabel('Country')
plt.ylabel('Number of Reviews')
plt.show()
